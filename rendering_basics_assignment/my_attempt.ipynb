{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6940b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pytorch3d\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from starter.utils import get_device, get_mesh_renderer, load_cow_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de97d97",
   "metadata": {},
   "source": [
    "### 360-degree render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3994516",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "cow_path=\"data/cow.obj\"\n",
    "image_size=256\n",
    "color=[0.7, 0.7, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8c088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = get_mesh_renderer(image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726ec17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vertices, faces, and textures.\n",
    "vertices, faces = load_cow_mesh(cow_path)\n",
    "vertices = vertices.unsqueeze(0)  # (N_v, 3) -> (1, N_v, 3)\n",
    "faces = faces.unsqueeze(0)  # (N_f, 3) -> (1, N_f, 3)\n",
    "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "textures = textures * torch.tensor(color)  # (1, N_v, 3)\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ")\n",
    "mesh = mesh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5123122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the camera:\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=torch.eye(3).unsqueeze(0), T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    ")\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4905ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R torch.Size([36, 3, 3])\n",
      "T torch.Size([36, 3])\n"
     ]
    }
   ],
   "source": [
    "num_views = 36\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(\n",
    "    dist=3,\n",
    "    elev=0,\n",
    "    azim=np.linspace(-180, 180, num_views, endpoint=False),\n",
    ")\n",
    "print(\"R\", R.shape)\n",
    "print(\"T\", T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28e13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=R,\n",
    "    T=T,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6646f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = get_mesh_renderer(image_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd7b07a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = renderer(mesh.extend(num_views), cameras=many_cameras, lights=lights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb6d1e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 512, 512, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5186646",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_images = [(img[..., :3].cpu().numpy() * 255).astype(np.uint8) for img in images]  # List of images [(H, W, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747336cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 1000 // 30  # Convert FPS (frames per second) to duration (ms per frame)\n",
    "imageio.mimsave('my_gif.gif', my_images, duration=duration, loop=0)  # loop=0 for infinite loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbe1f2",
   "metadata": {},
   "source": [
    "### Dolly zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "982c62a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120.0000, 116.7143, 113.4286, 110.1429, 106.8571, 103.5714, 100.2857,\n",
       "         97.0000,  93.7143,  90.4286,  87.1429,  83.8571,  80.5714,  77.2857,\n",
       "         74.0000,  70.7143,  67.4286,  64.1429,  60.8571,  57.5714,  54.2857,\n",
       "         51.0000,  47.7143,  44.4286,  41.1429,  37.8571,  34.5714,  31.2857,\n",
       "         28.0000,  24.7143,  21.4286,  18.1429,  14.8571,  11.5714,   8.2857,\n",
       "          5.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(120, 5, num_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b160fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131ef43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fovs = torch.linspace(25, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d059c25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(25.), 25.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fovs[0], fovs[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc69f0",
   "metadata": {},
   "source": [
    "### Constructing a tetrahedron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6107385",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = torch.tensor([[0,1,0], \n",
    "                         [3, 0, 0],\n",
    "                         [0, 2, 0],\n",
    "                         [1, 1, 3]], dtype=torch.float32, device=device).unsqueeze(0)  \n",
    "\n",
    "faces = torch.tensor([[0, 1, 2],\n",
    "                      [0, 1, 3],\n",
    "                      [0, 2, 3],\n",
    "                      [1, 2, 3]], dtype=torch.int64, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e9b4fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297909/4186888042.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  textures = textures * torch.tensor(color)\n"
     ]
    }
   ],
   "source": [
    "color = torch.tensor([[0.2, 0.5, 1.0]], device=device)\n",
    "textures = torch.ones_like(vertices)\n",
    "textures = textures * torch.tensor(color)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures)\n",
    ")\n",
    "mesh = mesh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfc08aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the camera:\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=torch.eye(3).unsqueeze(0), T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    ")\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84623d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = 36\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(\n",
    "    dist=5,\n",
    "    elev=30,\n",
    "    azim=np.linspace(-180, 180, num_views, endpoint=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6ccf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=R,\n",
    "    T=T,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fbb1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = get_mesh_renderer(image_size=512)\n",
    "images = renderer(mesh.extend(num_views), cameras=many_cameras, lights=lights)\n",
    "my_images = [(img[..., :3].cpu().numpy() * 255).astype(np.uint8) for img in images]  # List of images [(H, W, 3)]\n",
    "duration = 1000 // 30  # Convert FPS (frames per second) to duration (ms per frame)\n",
    "imageio.mimsave('tetrahedron.gif', my_images, duration=duration, loop=0)  # loop=0 for infinite loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45002360",
   "metadata": {},
   "source": [
    "### Constructing a cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = torch.tensor([[0, 0, 0], #0 \n",
    "                         [1, 0, 0], #1\n",
    "                         [1, 0, -1], #2\n",
    "                         [0, 0, -1], #3\n",
    "                         [0, 1, 0], #4\n",
    "                         [1, 1, 0], #5\n",
    "                         [1, 1, -1], #6\n",
    "                         [0, 1, -1]], dtype=torch.float32, device=device).unsqueeze(0) #7\n",
    "\n",
    "# mind the winding order, when looking from outside the cube vertices should be counter-clockwise\n",
    "faces = torch.tensor([[0, 1, 2], [0, 2, 3],\n",
    "                      [0, 5, 4], [0, 1, 5],\n",
    "                      [0, 7, 4], [0, 3, 7],\n",
    "                      [1, 2, 5], [2, 6, 5],\n",
    "                      [2, 6, 3], [7, 3, 6],\n",
    "                      [4, 7, 5], [7, 6, 5]], dtype=torch.int64, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dea7baf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297909/4186888042.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  textures = textures * torch.tensor(color)\n"
     ]
    }
   ],
   "source": [
    "color = torch.tensor([[0.2, 0.5, 1.0]], device=device)\n",
    "textures = torch.ones_like(vertices)\n",
    "textures = textures * torch.tensor(color)\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures)\n",
    ")\n",
    "mesh = mesh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1392ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the camera:\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=torch.eye(3).unsqueeze(0), T=torch.tensor([[0, 0, 3]]), fov=60, device=device\n",
    ")\n",
    "\n",
    "# Place a point light in front of the cow.\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3661ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = 36\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(\n",
    "    dist=5,\n",
    "    elev=30,\n",
    "    azim=np.linspace(-180, 180, num_views, endpoint=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "15ac101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_cameras = pytorch3d.renderer.FoVPerspectiveCameras(\n",
    "    R=R,\n",
    "    T=T,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7637232",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = get_mesh_renderer(image_size=512)\n",
    "images = renderer(mesh.extend(num_views), cameras=many_cameras, lights=lights)\n",
    "my_images = [(img[..., :3].cpu().numpy() * 255).astype(np.uint8) for img in images]  # List of images [(H, W, 3)]\n",
    "duration = 1000 // 30  # Convert FPS (frames per second) to duration (ms per frame)\n",
    "imageio.mimsave('cube.gif', my_images, duration=duration, loop=0)  # loop=0 for infinite loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79613f26",
   "metadata": {},
   "source": [
    "### Re-texturing a mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e9a99efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices, faces = load_cow_mesh(cow_path)\n",
    "vertices = vertices.unsqueeze(0)\n",
    "faces = faces.unsqueeze(0)  \n",
    "\n",
    "vertices = vertices.to(device)\n",
    "faces = faces.to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c30ca1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "colour1 = torch.tensor([1.0, 0.0, 0.0], device=device)  \n",
    "colour2 = torch.tensor([0.0, 0.0, 1.0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b125fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_coords = vertices[0, :, 2]\n",
    "z_min = z_coords.min()\n",
    "z_max = z_coords.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a39c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = (z_coords - z_min) / (z_max - z_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d8cec209",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_colors = alpha[:, None] * colour2 + (1 - alpha[:, None]) * colour1\n",
    "vertex_colors = vertex_colors.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d12121b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(vertex_colors)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "47f5e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_views = 36\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(\n",
    "    dist=3, elev=0, azim=np.linspace(-180, 180, num_views, endpoint=False)\n",
    ")\n",
    "many_cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "images = renderer(mesh.extend(num_views), cameras=many_cameras, lights=lights)\n",
    "my_images = [(img[..., :3].cpu().numpy() * 255).astype(np.uint8) for img in images]\n",
    "imageio.mimsave('gradient_cow.gif', my_images, duration=1000//30, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b77d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
